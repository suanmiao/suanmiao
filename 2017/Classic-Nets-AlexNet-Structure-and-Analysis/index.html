<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title>Classic Nets & AlexNet Structure and Analysis · Quinn</title><meta name="description" content="Layer Construction:
Layer 0: Input image* Size: 227 x 227 x 3
* Note that in the paper referenced above, the network diagram has 224x224x3 printed which appears to be a typo.

Layer 1: Convolution with 96 filters, size 11×11, stride 4, pa"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="/css/useso.css"></head><body><header><a href="/" class="logo-link"><img src="/img/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/archives/index.html" target="_self" class="nav-list-link">BLOGS</a></li><li class="nav-list-item"><a href="/index.html" target="_self" class="nav-list-link">PORTFOLIO</a></li><li class="nav-list-item"><a href="https://github.com/suanmiao" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">Classic Nets & AlexNet Structure and Analysis</h1><div class="post-meta"><div class="post-time">Jan 21, 2017</div></div><div class="post-content"><h3 id="Layer_Construction:">Layer Construction:</h3><ul>
<li><strong>Layer 0: Input image</strong><pre><code>* Size: <span class="number">227</span> x <span class="number">227</span> x <span class="number">3</span>
* Note that <span class="operator">in</span> <span class="operator">the</span> paper referenced above, <span class="operator">the</span> network diagram has <span class="number">224</span>x224x3 printed which appears <span class="built_in">to</span> be <span class="operator">a</span> typo.
</code></pre></li>
<li><strong>Layer 1: Convolution with 96 filters, size 11×11, stride 4, padding 0</strong><pre><code>* Size: <span class="number">55</span> x <span class="number">55</span> x <span class="number">96</span>
* (<span class="number">227</span>-<span class="number">11</span>)/<span class="number">4</span> + <span class="number">1</span> = <span class="number">55</span> is the size of the outcome
* <span class="number">96</span> depth because <span class="number">1</span> <span class="built_in">set</span> denotes <span class="number">1</span> filter and there are <span class="number">96</span> filters
</code></pre></li>
<li><strong>Layer 2: Max-Pooling with 3×3 filter, stride 2</strong><pre><code>* Size: <span class="number">27</span> x <span class="number">27</span> x <span class="number">96</span>
* (<span class="number">55</span> – <span class="number">3</span>)/<span class="number">2</span> + <span class="number">1</span> = <span class="number">27</span> is size of outcome
* depth is same as before, i.e. <span class="number">96</span> because pooling is done independently on each layer
</code></pre></li>
<li><strong>Layer 3: Convolution with 256 filters, size 5×5, stride 1, padding 2</strong><pre><code>* Size: <span class="number">27</span> x <span class="number">27</span> x <span class="number">256</span>
* Because <span class="keyword">of</span> padding <span class="keyword">of</span> (<span class="number">5</span>-<span class="number">1</span>)/<span class="number">2</span>=<span class="number">2</span>, <span class="keyword">the</span> original size <span class="keyword">is</span> restored
* <span class="number">256</span> depth because <span class="keyword">of</span> <span class="number">256</span> filters
</code></pre></li>
<li><strong>Layer 4: Max-Pooling with 3×3 filter, stride 2</strong><pre><code>* Size: <span class="number">13</span> x <span class="number">13</span> x <span class="number">256</span>
* (<span class="number">27</span> – <span class="number">3</span>)/<span class="number">2</span> + <span class="number">1</span> = <span class="number">13</span> is size of outcome
* Depth is same as before, i.e. <span class="number">256</span> because pooling is done independently on each layer
</code></pre><strong>* Layer 5: Convolution with 384 filters, size 3×3, stride 1, padding 1</strong><pre><code>* Size: <span class="number">13</span> x <span class="number">13</span> x <span class="number">384</span>
* Because <span class="keyword">of</span> padding <span class="keyword">of</span> (<span class="number">3</span>-<span class="number">1</span>)/<span class="number">2</span>=<span class="number">1</span>, <span class="keyword">the</span> original size <span class="keyword">is</span> restored
* <span class="number">384</span> depth because <span class="keyword">of</span> <span class="number">384</span> filters
</code></pre></li>
<li><strong>Layer 6: Convolution with 384 filters, size 3×3, stride 1, padding 1</strong><pre><code>* Size: <span class="number">13</span> x <span class="number">13</span> x <span class="number">384</span>
* Because <span class="keyword">of</span> padding <span class="keyword">of</span> (<span class="number">3</span>-<span class="number">1</span>)/<span class="number">2</span>=<span class="number">1</span>, <span class="keyword">the</span> original size <span class="keyword">is</span> restored
* <span class="number">384</span> depth because <span class="keyword">of</span> <span class="number">384</span> filters
</code></pre></li>
<li><strong>Layer 7: Convolution with 256 filters, size 3×3, stride 1, padding 1</strong><pre><code>* Size: <span class="number">13</span> x <span class="number">13</span> x <span class="number">256</span>
* Because <span class="keyword">of</span> padding <span class="keyword">of</span> (<span class="number">3</span>-<span class="number">1</span>)/<span class="number">2</span>=<span class="number">1</span>, <span class="keyword">the</span> original size <span class="keyword">is</span> restored
* <span class="number">256</span> depth because <span class="keyword">of</span> <span class="number">256</span> filters
</code></pre></li>
<li><strong>Layer 8: Max-Pooling with 3×3 filter, stride 2</strong><pre><code>* Size: <span class="number">6</span> x <span class="number">6</span> x <span class="number">256</span>
* (<span class="number">13</span> – <span class="number">3</span>)/<span class="number">2</span> + <span class="number">1</span> = <span class="number">6</span> is size of outcome
* Depth is same as before, i.e. <span class="number">256</span> because pooling is done independently on each layer
</code></pre></li>
<li><strong>Layer 9: Fully Connected with 4096 neuron</strong><pre><code>* In this later, <span class="keyword">each</span> <span class="operator">of</span> <span class="operator">the</span> <span class="number">6</span>x6x256=<span class="number">9216</span> pixels are fed <span class="keyword">into</span> <span class="keyword">each</span> <span class="operator">of</span> <span class="operator">the</span> <span class="number">4096</span> neurons <span class="operator">and</span> weights determined <span class="keyword">by</span> back-propagation.
</code></pre></li>
<li><strong>Layer 10: Fully Connected with 4096 neuron</strong><pre><code>* Similar <span class="keyword">to</span> layer <span class="comment">#9</span>
</code></pre></li>
<li><strong>Layer 11: Fully Connected with 1000 neurons</strong><pre><code>* This <span class="keyword">is</span> the <span class="keyword">last</span> layer <span class="built_in">and</span> <span class="built_in">has</span> <span class="number">1000</span> neurons because IMAGENET data <span class="built_in">has</span> <span class="number">1000</span> classes <span class="keyword">to</span> <span class="keyword">be</span> predicted.
</code></pre></li>
</ul>
<a id="more"></a>
<h3 id="Parameter_size:">Parameter size:</h3><p><strong>Layer 0:</strong></p>
<ul>
<li>Memory: 227 x 227 x 3</li>
</ul>
<p><strong>Layer 1: (conv + ReLU +  LRN)</strong></p>
<ul>
<li>Memory: 55 x 55 x 96 x 3 (because of ReLU and LRN)</li>
<li>Weights: 11 x 11 x 3 x 96</li>
</ul>
<p><strong>Layer 2: (pooling)</strong></p>
<ul>
<li>Memory: 27 x 27 x 96 </li>
</ul>
<p><strong>Layer 3: (conv + ReLU + LRN)</strong></p>
<ul>
<li>Memory: 27 x 27 x 256 x 3 (because of ReLU and LRN)</li>
<li>Weights: 5 x 5 x 96 x 256</li>
</ul>
<p><strong>Layer 4: (pooling)</strong></p>
<ul>
<li>Memory: 13 x 13 x 256</li>
</ul>
<p><strong>Layer 5: (conv + ReLU)</strong></p>
<ul>
<li>Memory: 13 x 13 x 384 x 2 (because of ReLU)</li>
<li>Weights: 3 x 3 x 256 x 384</li>
</ul>
<p><strong>Layer 6: (conv + ReLU)</strong></p>
<ul>
<li>Memory: 13 x 13 x 384 x 2 (because of ReLU)</li>
<li>Weights: 3 x 3 x 384 x 384</li>
</ul>
<p><strong>Layer 7: (conv + ReLU)</strong></p>
<ul>
<li>Memory: 13 x 13 x 256 x 2 (because of ReLU)</li>
<li>Weights: 3 x 3 x 384 x 256</li>
</ul>
<p><strong>Layer 8: (pooling)</strong></p>
<ul>
<li>Memory: 6 x 6 x 256</li>
</ul>
<p><strong>Layer 9: (FC + ReLU + Dropout)</strong></p>
<ul>
<li>Memory: 4096 x 3 (because of ReLU and Dropout)</li>
<li>Weights: 4096 x (6 x 6 x 256)</li>
</ul>
<p><strong>Layer 9: (FC + ReLU + Dropout)</strong></p>
<ul>
<li>Memory: 4096 x 3 (because of ReLU and Dropout)</li>
<li>Weights: 4096 x 4096</li>
</ul>
<p><strong>Layer 10: (FC)</strong></p>
<ul>
<li>Memory: 1000</li>
<li>Weights: 4096 x 1000</li>
</ul>
<p><strong>Total (label and softmax not included)</strong></p>
<ul>
<li>Memory: 2.24 million</li>
<li>Weights: 62.37 million </li>
</ul>
<p><strong>Reference:</strong></p>
<ul>
<li><a href="https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/" target="_blank" rel="external">https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/</a></li>
<li><a href="http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/myalexnet_forward.py" target="_blank" rel="external">http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/myalexnet_forward.py</a></li>
<li><a href="http://hacker.duanshishi.com/?p=1661" target="_blank" rel="external">http://hacker.duanshishi.com/?p=1661</a></li>
<li><a href="https://github.com/tflearn/tflearn/blob/master/examples/images/alexnet.py" target="_blank" rel="external">https://github.com/tflearn/tflearn/blob/master/examples/images/alexnet.py</a></li>
<li><a href="http://www.jeyzhang.com/tensorflow-learning-notes-2.html" target="_blank" rel="external">http://www.jeyzhang.com/tensorflow-learning-notes-2.html</a></li>
</ul>
<h3 id="Visualization">Visualization</h3><ul>
<li><a href="https://dgschwend.github.io/netscope/#/preset/alexnet" target="_blank" rel="external">https://dgschwend.github.io/netscope/#/preset/alexnet</a></li>
</ul>
<h3 id="Analysis">Analysis</h3><p> <strong>ReLU layer for activation</strong></p>
<ul>
<li>ReLUs have the desirable property that they do not require input normalization to prevent them from saturating</li>
<li>this make the network converges faster than original tanh activation function</li>
</ul>
<p><strong>LRN</strong></p>
<ul>
<li>But with ReLU, the output of that layer is not normalized as tanh or sigmoid. So we need a normalization layer to make the output normalized</li>
</ul>
<h4 id="There_are_two_ways_to_reduce_overfitting:_more_data;_more_robust_network_structure-">There are two ways to reduce overfitting: more data; more robust network structure.</h4><h4 id="Data_Augmentation">Data Augmentation</h4><ul>
<li>Translation and horizontal reflection</li>
<li>later the intensities of RGB channels</li>
</ul>
<h4 id="Dropout">Dropout</h4><p><strong>Incentive:</strong></p>
<ul>
<li>Accomplish the effect of ‘combining prediction of many different models’, which can reduce test errors</li>
<li>But this requires many neural networks or one neural network to be trained in different ways</li>
</ul>
<p><strong>Method:</strong></p>
<ul>
<li>Randomly set 50% of output of each hidden neuron to be zero</li>
<li>Thus these neurons do not participated in forward pass and back-propagation</li>
</ul>
<p><strong>Effect</strong></p>
<ul>
<li>This effect is just like each time a different architecture, but all these architectures share same weights</li>
</ul>
<p><strong>Advantage</strong></p>
<ul>
<li>Because neurons can not rely on the prescence of particular other neurons, so it reduces co-adaptation of neurons</li>
<li>It forces neurons to learn more robust features</li>
</ul>
</div></article></div></section><footer><div class="paginator"><a class="prev"> </a><a href="/2016/Project-Wearable-Cognitive-Assistant/" class="next">下一篇</a></div></footer><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-71082558-1",'auto');ga('send','pageview');</script><script src="https://cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>